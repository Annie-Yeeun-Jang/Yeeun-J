{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annie-Yeeun-Jang/Yeeun-J/blob/master/BERT_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJbo6tgsmBMq",
        "outputId": "6eaa69a9-f2f4-4b86-ea7d-820bf94bd9d6"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('./gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OBmC3UmYaRP"
      },
      "source": [
        "# Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVCytM2-YWWR"
      },
      "source": [
        "#source_folder = '/content/drive/My Drive/transformers/Data'\n",
        "destination_folder = '/content/gdrive/My Drive/transformers/Model'\n",
        "source_folder = '/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahtu1eni7B7h"
      },
      "source": [
        "#/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc/YELC_preprocessed_A.txt"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKVe4gRJBMja"
      },
      "source": [
        "with open('/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc/YELC_preprocessed_A.txt', encoding = 'UTF-8', newline = '\\n',errors='ignore') as f:\n",
        "  nonn = f.readlines()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX3CBlwrBie1"
      },
      "source": [
        "with open('/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc/YELC_preprocessed_C.txt', encoding = 'UTF-8', newline = '\\n',errors='ignore') as f:\n",
        "  nat = f.readlines()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0AebHjZBfw9"
      },
      "source": [
        "nonn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1j7XfjcBmM3"
      },
      "source": [
        "nat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0JQjKMgCBlq"
      },
      "source": [
        "nonn_df = pd.DataFrame({'text':nonn, 'label': 0})\n",
        "native_df = pd.DataFrame({'text':nat, 'label': 1})"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scICH_RKCL8j"
      },
      "source": [
        "all_df = pd.concat([nonn_df,native_df], axis = 0)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "XkcgXnRPCl9T",
        "outputId": "c012634b-81aa-4158-9c66-204110f34ff5"
      },
      "source": [
        "all_df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>My favorite teacher's name is Jae-Won Jung my ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>She often tell us interesting story. \\n</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So she's lesson was not bored. \\n</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I don't think that all Korean men should be fo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>my best friend name is satbyul. \\n</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>This is a very effective method because studen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>953</th>\n",
              "      <td>In conclusion, physical punishment should be a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>954</th>\n",
              "      <td>Schools and teachers should always have the st...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955</th>\n",
              "      <td>Then, physical punishment would not be linked ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>Instead it would play its true role in the soc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21062 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  label\n",
              "0    My favorite teacher's name is Jae-Won Jung my ...      0\n",
              "1              She often tell us interesting story. \\n      0\n",
              "2                    So she's lesson was not bored. \\n      0\n",
              "3    I don't think that all Korean men should be fo...      0\n",
              "4                   my best friend name is satbyul. \\n      0\n",
              "..                                                 ...    ...\n",
              "952  This is a very effective method because studen...      1\n",
              "953  In conclusion, physical punishment should be a...      1\n",
              "954  Schools and teachers should always have the st...      1\n",
              "955  Then, physical punishment would not be linked ...      1\n",
              "956  Instead it would play its true role in the soc...      1\n",
              "\n",
              "[21062 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppvIlaThCnmM"
      },
      "source": [
        "shuff_df = all_df.sample(frac=1).reset_index(drop=True) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h84T6k27xMgM"
      },
      "source": [
        "shuff_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIdOr-3bC9-X"
      },
      "source": [
        "train_df = shuff_df[:int(len(shuff_df)*0.6)].reset_index(drop=True) \n",
        "val_df = shuff_df[int(len(shuff_df)*0.6):int(len(shuff_df)*0.7)].reset_index(drop=True)\n",
        "test_df = shuff_df[int(len(shuff_df)*0.7):].reset_index(drop=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V87-tG57DYVT"
      },
      "source": [
        "train_df.to_csv('/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc/yelc_train.csv')\n",
        "val_df.to_csv('/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc/yelc_val.csv')\n",
        "test_df.to_csv('/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc/yelc_test.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyZ65LZ8YgtG"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY4NR2cVZNFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f970c158-1cc6-4e4f-f170-7361a34fb488"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.9MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGC6DLQv7PuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5fb362-a0dd-4865-842e-ad06c6b38956"
      },
      "source": [
        "!pip install -U torchtext==0.8.0"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/8a/e09b9b82d4dd676f17aa681003a7533765346744391966dec0d5dba03ee4/torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.9.0+cu102)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed torchtext-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-e2B79UlzR2"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3z1qC9IiIsI",
        "outputId": "71385a56-2ff9-4b25-c9eb-5b4b0789bda6"
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl7rqQ4uZPiv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDBAx2Q-ZMl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93111d81-93ed-4e57-b74b-c287f58fd138"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cpu'\n",
        "print(device)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqOJAYCiYlZs"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF1DCVrCh6_d"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPRlMxwcvJfR"
      },
      "source": [
        "destination_folder = '/content/gdrive/MyDrive/transformer-ckpt'\n",
        "data_folder = '/content/gdrive/Shareddrives/text_conf/dataset/preprocessed/yelc/'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnP1xfJq4lTo",
        "outputId": "fc74884e-9766-439b-eea1-5baf14646f97"
      },
      "source": [
        "print(vars(train[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'label': '0', 'text': [101, 1045, 5993, 2008, 2111, 2442, 2224, 2037, 2613, 2171, 2006, 1996, 4274, 1012, 102]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRSdmqyH05f5"
      },
      "source": [
        "for l, d in train_iter:\n",
        "  print(\"data: \",d) # 이건 뭐에 해당하는거지...?\n",
        "  print(\"label: \", l)\n",
        "  print(\"*\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdGccqrAZYjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bbc593-6006-4653-d30d-d47796e95491"
      },
      "source": [
        "# Model parameter\n",
        "MAX_SEQ_LEN = 128\n",
        "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
        "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
        "\n",
        "# Fields\n",
        "\n",
        "#label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(use_vocab=False, tokenize=tokenizer.encode,is_target = True, lower=False, include_lengths=False, batch_first=True,\n",
        "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
        "#fields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]\n",
        "fields = [('label', label_field), ('text', text_field)] #fields = df columns\n",
        "\n",
        "# TabularDataset\n",
        "train, valid, test = TabularDataset.splits(path=data_folder, train='yelc_train.csv',validation= 'yelc_val.csv', test='yelc_test.csv', format='CSV', fields=fields, skip_header=True)\n",
        "\n",
        "# Iterators\n",
        "train_iter = BucketIterator(train, batch_size=16, device=device, shuffle=True)\n",
        "valid_iter = BucketIterator(valid, batch_size=16, device=device, shuffle=True)\n",
        "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n",
        "#train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text), device=device, train=True, sort=True, sort_within_batch=True)\n",
        "#valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text), device=device, train=True, sort=True, sort_within_batch=True)\n",
        "#test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbvxnbr15iiF",
        "outputId": "ad8bb2a7-babe-4f27-b9c9-92b5d979f04a"
      },
      "source": [
        "next(iter(train_iter)).label #라벨 왜이럼??????"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 7490.,  8205.,    81., 10785.,  4258., 10346.,  5334.,  6211., 10002.,\n",
              "         2832., 11920.,  4417.,  9546.,  6564.,  8145.,  8474.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jw6wG1O40H1"
      },
      "source": [
        "print(text_field.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-lWIMaYnsA"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RkcXCHSph1_"
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        options_name = \"bert-base-uncased\"\n",
        "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
        "\n",
        "    def forward(self, text, label):\n",
        "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
        "\n",
        "        return loss, text_fea"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z81slSELYqO1"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRpTJUGhklDv"
      },
      "source": [
        "# Save and Load Functions\n",
        "\n",
        "def save_checkpoint(save_path, model, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81w1lahhkozO"
      },
      "source": [
        "# Training Function\n",
        "\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(),\n",
        "          train_loader = train_iter,\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_iter) // 2,\n",
        "          file_path = destination_folder,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "    \n",
        "    # initialize running values\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    # training loop\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for (labels, text) in train_loader:\n",
        "            labels = labels.type(torch.LongTensor)           \n",
        "            labels = labels.to(device)\n",
        "            text = text.type(torch.LongTensor)  \n",
        "            text = text.to(device)\n",
        "            print(\"labels shape:{} , text shape: {}\".format(labels, text))\n",
        "            output = model(text, labels)\n",
        "            print(\"output shape: \", output.shape)\n",
        "\n",
        "            loss, _ = output\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update running values\n",
        "            running_loss += loss.item()\n",
        "            global_step += 1\n",
        "\n",
        "            # evaluation step\n",
        "            if global_step % eval_every == 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():                    \n",
        "\n",
        "                    # validation loop\n",
        "                    for (labels, text) in valid_loader:\n",
        "                        labels = labels.type(torch.LongTensor)           \n",
        "                        labels = labels.to(device)\n",
        "                        text = text.type(torch.LongTensor)  \n",
        "                        text = text.to(device)\n",
        "                        output = model(text, labels)\n",
        "                        \n",
        "                        loss, _ = output\n",
        "                        \n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                # evaluation\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                # resetting running values\n",
        "                running_loss = 0.0                \n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                # print progress\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "                \n",
        "                # checkpoint\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
        "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    \n",
        "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHdi_cyEvC9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90828267-d53b-45d4-96dc-52b985d8eb13"
      },
      "source": [
        "model = BERT().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "train(model=model, optimizer=optimizer)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "labels shape:tensor([ 5545,  7839,  9650,  6435, 10140,  2914,  1493,  8512,  7285,  2584,\n",
            "        12302,  1300,  3473, 10588, 10465,  3661]) , text shape: tensor([[ 101, 2017, 2031,  ...,    0,    0,    0],\n",
            "        [ 101, 2096, 1045,  ...,    0,    0,    0],\n",
            "        [ 101, 2036, 2493,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2065, 2162,  ...,    0,    0,    0],\n",
            "        [ 101, 3558, 7750,  ...,    0,    0,    0],\n",
            "        [ 101, 2061, 1045,  ...,    0,    0,    0]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-e4474bff9c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-2dd479a3c579>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, num_epochs, eval_every, file_path, best_valid_loss)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels shape:{} , text shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-71a85d6c1fce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, label)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_fea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1121\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 5545 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JaIsEttvMeO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "fb7be7bc-bb81-4151-9fc8-ec1e26a0ae4f"
      },
      "source": [
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-b5abe49452b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_steps_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/metrics.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_steps_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_steps_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Global Steps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-840b79b70e76>\u001b[0m in \u001b[0;36mload_metrics\u001b[0;34m(load_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model loaded from <== {load_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/transformers/Model/metrics.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qLyO8EWwJEo"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBhAW6rwKly"
      },
      "source": [
        "# Evaluation Function\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for (labels, text) in test_loader:\n",
        "\n",
        "                labels = labels.type(torch.LongTensor)           \n",
        "                labels = labels.to(device)\n",
        "                text = text.type(torch.LongTensor)  \n",
        "                text = text.to(device)\n",
        "                output = model(text, labels)\n",
        "\n",
        "                _, output = output\n",
        "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
        "                y_true.extend(labels.tolist())\n",
        "    \n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "\n",
        "    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n",
        "    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLlijSLcxv2j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "e301fd54-912a-4b75-fd8f-c79379d2a008"
      },
      "source": [
        "best_model = BERT().to(device)\n",
        "\n",
        "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
        "\n",
        "evaluate(best_model, test_iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded from <== /content/drive/My Drive/transformers/Model/model.pt\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.9804    0.9585    0.9693       313\n",
            "           0     0.9600    0.9811    0.9705       318\n",
            "\n",
            "    accuracy                         0.9699       631\n",
            "   macro avg     0.9702    0.9698    0.9699       631\n",
            "weighted avg     0.9701    0.9699    0.9699       631\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}